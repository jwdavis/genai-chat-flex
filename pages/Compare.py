from langchain.schema import ChatMessage
from langchain_openai import OpenAI, ChatOpenAI
from langchain_google_vertexai import VertexAI

from config import secrets

import asyncio
import streamlit as st

st.set_page_config(
    page_title='ROI GenAI Chat',
    page_icon='./static/ROISquareLogo.png',
    layout="wide"
)

MODELS = {
    'PaLMv2': 'text-bison',
    'Gemini-Pro': 'gemini-pro',
    'GPT-3.5': 'gpt-3.5-turbo-instruct',
    'GPT-4 Turbo': 'gpt-4',
    'Codey': 'code-bison'
}


def show_sidebar():
    """
    Displays the sidebar
    """
    from streamlit_extras.add_vertical_space import add_vertical_space
    with st.sidebar:
        add_vertical_space(1)
        st.link_button(
            label="Watch Overview Video",
            url="https://drive.google.com/file/d/1AUS4iz22fvuj3xRx38JI3YDX06BWDzU_/view?usp=sharing",
            type="primary")


def show_intro():
    """
    Displays the introduction section of the ROI Training GenAI Chat page.
    """
    st.image(
        "https://www.roitraining.com/wp-content/uploads/2017/02/ROI-logo.png",
        width=300
    )
    st.title("GenAI Text Generation Comparison")
    # st.divider()


async def get_response(model_key: str, prompt: str) -> str:
    """
    Retrieves a response from the specified language model based on the given prompt.

    Args:
        model_key (str): The key of the language model to use.
        prompt (str): The prompt for the language model.

    Returns:
        str: The response generated by the language model.
    """
    prompt = f"""
    You are a large language model. Follow the user's instructions 
        carefully, and where appropriate explain your reasoning. 
        Respond using markdown. Please respond to the following prompt:

    {prompt}
    """
    if model_key in ['PaLMv2', 'Gemini-Pro', 'Codey']:
        model_name = MODELS[model_key]
        llm = VertexAI(model_name=model_name)
        response = await llm.ainvoke(prompt)
        return response
    elif model_key == 'GPT-3.5':
        model_name = MODELS[model_key]
        chat = OpenAI(
            openai_api_key=secrets['openai_api_key'],
            model=model_name
        )
        response = await chat.ainvoke(prompt)
        return response
    elif model_key == 'GPT-4 Turbo':
        model_name = MODELS[model_key]
        chat = ChatOpenAI(
            openai_api_key=secrets['openai_api_key'],
            model=model_name
        )
        messages = [
            ChatMessage(role="assistant", content="How can I help you?"),
            ChatMessage(role="user", content=prompt)
        ]
        response = await chat.ainvoke(messages)
        return response.content
    
async def update_tab(tab, result,empty):
    empty.empty()
    tab.markdown(result, unsafe_allow_html=True)

async def main():
    show_sidebar()
    show_intro()

    tabs = st.tabs(MODELS.keys())
    empties = []
    prompt = st.chat_input("Your prompt")
    if prompt:
        tasks = []
        for i, model_key in enumerate(MODELS.keys()):
            with tabs[i]:
                empty = st.empty()
                with empty:
                    # st.write(f"Loading...{i}")
                    st.status(f"Getting response from {model_key}...")
                empties.append(empty)

            async def update_tab_coroutine(tab, result, empty):
                await update_tab(tab, result, empty)

            task = asyncio.create_task(get_response(model_key, prompt))
            task.add_done_callback(
                lambda t, i=i: asyncio.create_task(
                    update_tab_coroutine(tabs[i], t.result(), empties[i])
                )
            )
            tasks.append(task)
        await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
