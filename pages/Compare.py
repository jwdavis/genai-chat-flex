from langchain.schema import ChatMessage
from langchain_openai import OpenAI, ChatOpenAI
from langchain_google_vertexai import VertexAI
from config import secrets
import streamlit as st
import sys
sys.path.append("..")


st.set_page_config(
    page_title='ROI GenAI Chat',
    page_icon='./static/ROISquareLogo.png',
    layout="wide"
)

MODELS = {
    'PaLMv2': 'text-bison',
    'Gemini-Pro': 'gemini-pro',
    'GPT-3.5': 'gpt-3.5-turbo-instruct',
    'GPT-4 Turbo': 'gpt-4',
    'Codey': 'code-bison'
}


def show_intro():
    """
    Displays the introduction section of the ROI Training GenAI Chat page.
    """
    st.image(
        "https://www.roitraining.com/wp-content/uploads/2017/02/ROI-logo.png",
        width=300
    )
    st.title("ROI Training GenAI Chat")
    st.divider()


def get_response(model_key: str, prompt: str) -> str:
    """
    Retrieves a response from the specified language model based on the given prompt.

    Args:
        model_key (str): The key of the language model to use.
        prompt (str): The prompt for the language model.

    Returns:
        str: The response generated by the language model.
    """
    prompt = f"""
    You are a helpful assistant. Please keep your answers brief, under 300 
    words. Please respond to the following prompt:

    {prompt}
    """
    if model_key in ['PaLMv2', 'Gemini-Pro', 'Codey']:
        model_name = MODELS[model_key]
        llm = VertexAI(model_name=model_name)
        response = llm.invoke(prompt)
        return response
    elif model_key == 'GPT-3.5':
        model_name = MODELS[model_key]
        chat = OpenAI(
            openai_api_key=secrets['openai_api_key'],
            model=model_name
        )
        response = chat.invoke(prompt)
        return response
    elif model_key == 'GPT-4 Turbo':
        model_name = MODELS[model_key]
        chat = ChatOpenAI(
            openai_api_key=secrets['openai_api_key'],
            model=model_name
        )
        messages = [
            ChatMessage(role="assistant", content="How can I help you?"),
            ChatMessage(role="user", content=prompt)
        ]
        response = chat.invoke(messages)
        return response.content


def display_responses(containers: list, responses: list):
    """
    Display the responses in the given containers.

    Args:
        containers (list): A list of containers to display the responses in.
        responses (list): A list of responses to be displayed.

    Returns:
        None
    """
    for container in containers:
        try:
            container.write(responses.pop(0))
        except IndexError:
            container.write("TBD")


show_intro()

containers = []
cols = [col for col in st.columns(2)]
num_models = len(MODELS)
for i, model in enumerate(MODELS.keys()):
    container = cols[i % 2].container(border=True)
    container.markdown(f"<h6>{model}</h6>", unsafe_allow_html=True)
    containers.append(container)

if num_models % 2 != 0:
    container = cols[1].container(border=True)
    container.markdown("<h6>TBD</h6>", unsafe_allow_html=True)
    containers.append(container)

prompt = st.chat_input("Your prompt")
if prompt:
    with st.spinner("Getting responses"):
        responses = []
        for i, model_key in enumerate(MODELS.keys()):
            responses.append(get_response(model_key, prompt))
        display_responses(containers, responses)
