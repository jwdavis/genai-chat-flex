from langchain.schema import ChatMessage
from langchain_openai import OpenAI, ChatOpenAI
from langchain_google_vertexai import VertexAI

from config import secrets

import asyncio
import streamlit as st
import sys
sys.path.append("..")


st.set_page_config(
    page_title='ROI GenAI Chat',
    page_icon='./static/ROISquareLogo.png',
    layout="wide"
)

MODELS = {
    'PaLMv2': 'text-bison',
    'Gemini-Pro': 'gemini-pro',
    'GPT-3.5': 'gpt-3.5-turbo-instruct',
    'GPT-4 Turbo': 'gpt-4',
    'Codey': 'code-bison'
}


def show_sidebar():
    """
    Displays the sidebar with a selectbox to choose a model.
    Updates the session state with the selected model type.
    """
    from streamlit_extras.add_vertical_space import add_vertical_space
    with st.sidebar:
        add_vertical_space(1)
        st.link_button(
            label="Overview video",
            url="https://drive.google.com/file/d/1AUS4iz22fvuj3xRx38JI3YDX06BWDzU_/view?usp=sharing",
            type="primary")


def show_intro():
    """
    Displays the introduction section of the ROI Training GenAI Chat page.
    """
    st.image(
        "https://www.roitraining.com/wp-content/uploads/2017/02/ROI-logo.png",
        width=300
    )
    st.title("ROI Training GenAI Chat")
    st.divider()


async def get_response(model_key: str, prompt: str) -> str:
    """
    Retrieves a response from the specified language model based on the given prompt.

    Args:
        model_key (str): The key of the language model to use.
        prompt (str): The prompt for the language model.

    Returns:
        str: The response generated by the language model.
    """
    prompt = f"""
    You are a helpful assistant. Please keep your answers brief, under 300 
    words. Please respond to the following prompt:

    {prompt}
    """
    if model_key in ['PaLMv2', 'Gemini-Pro', 'Codey']:
        model_name = MODELS[model_key]
        llm = VertexAI(model_name=model_name)
        response = await llm.ainvoke(prompt)
        return response
    elif model_key == 'GPT-3.5':
        model_name = MODELS[model_key]
        chat = OpenAI(
            openai_api_key=secrets['openai_api_key'],
            model=model_name
        )
        response = await chat.ainvoke(prompt)
        return response
    elif model_key == 'GPT-4 Turbo':
        model_name = MODELS[model_key]
        chat = ChatOpenAI(
            openai_api_key=secrets['openai_api_key'],
            model=model_name
        )
        messages = [
            ChatMessage(role="assistant", content="How can I help you?"),
            ChatMessage(role="user", content=prompt)
        ]
        response = await chat.ainvoke(messages)
        return response.content


def display_responses(containers: list, responses: list):
    """
    Display the responses in the given containers.

    Args:
        containers (list): A list of containers to display the responses in.
        responses (list): A list of responses to be displayed.

    Returns:
        None
    """
    for container in containers:
        try:
            container.write(responses.pop(0))
        except IndexError:
            container.write("TBD")


async def main():
    show_sidebar()
    show_intro()

    containers = []
    cols = [col for col in st.columns(2)]
    num_models = len(MODELS)
    for i, model in enumerate(MODELS.keys()):
        container = cols[i % 2].container(height=400, border=True)
        container.markdown(f"<h6>{model}</h6>", unsafe_allow_html=True)
        containers.append(container)

    if num_models % 2 != 0:
        container = cols[1].container(height=400, border=True)
        container.markdown("<h6>TBD</h6>", unsafe_allow_html=True)
        containers.append(container)

    prompt = st.chat_input("Your prompt")
    if prompt:
        with st.spinner("Getting responses"):
            t1 = asyncio.create_task(get_response('PaLMv2', prompt))
            t2 = asyncio.create_task(get_response('Gemini-Pro', prompt))
            responses = [
                await t1,
                await t2
            ]
            tasks = []
            responses = []
            for i, model_key in enumerate(MODELS.keys()):
                tasks.append(asyncio.create_task(
                    get_response(model_key, prompt)))
            for task in tasks:
                responses.append(await task)
            display_responses(containers, responses)

if __name__ == "__main__":
    asyncio.run(main())
