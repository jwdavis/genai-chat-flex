from langchain.schema import ChatMessage
from langchain_openai import OpenAI, ChatOpenAI
from langchain_google_vertexai import VertexAI

from config import secrets, md_dict

import asyncio
import streamlit as st

st.set_page_config(
    page_title='ROI GenAI Chat',
    page_icon='./static/ROISquareLogo.png',
    layout="wide"
)

MODELS = {
    'PaLM': 'text-bison',
    'Gemini 1.0 Pro': 'gemini-1.0-pro',
    'GPT-3.5': 'gpt-3.5-turbo-instruct',
    'GPT-4 Turbo': 'gpt-4-turbo-preview',
    'Codey': 'code-bison',
    'Gemini 1.0 Pro Vision (soon)': 'gemini-1.0-pro-vision',
    'GPT-4 Turbo Vision (soon)': 'gpt-4-vision-preview',
}


def show_sidebar():
    """
    Displays the sidebar
    """
    from streamlit_extras.add_vertical_space import add_vertical_space
    with st.sidebar:
        add_vertical_space(1)
        st.link_button(
            label="Watch Overview Video",
            url="https://drive.google.com/file/d/1AUS4iz22fvuj3xRx38JI3YDX06BWDzU_/view?usp=sharing",
            type="primary")


def show_intro():
    """
    Displays the introduction section of the ROI Training GenAI Chat page.
    """
    st.image(
        "https://www.roitraining.com/wp-content/uploads/2017/02/ROI-logo.png",
        width=300
    )
    st.title("GenAI Text Generation Comparison")


async def get_response(model_key: str, prompt: str) -> str:
    """
    Retrieves a response from the specified language model based on the given prompt.

    Args:
        model_key (str): The key of the language model to use.
        prompt (str): The prompt for the language model.

    Returns:
        str: The response generated by the language model.
    """
    prompt = f"""
    You are a Generative AI model. Follow the user's instructions 
        carefully, and where appropriate explain your reasoning. 
        Respond using markdown. Please respond to the following prompt:

    {prompt}
    """
    if model_key in ['PaLM', 'Gemini 1.0 Pro', 'Codey']:
        model_name = MODELS[model_key]
        llm = VertexAI(model_name=model_name)
        response = await llm.ainvoke(prompt)
        return response
    elif model_key == 'GPT-3.5':
        model_name = MODELS[model_key]
        chat = OpenAI(
            openai_api_key=secrets['openai_api_key'],
            model=model_name
        )
        response = await chat.ainvoke(prompt)
        return response
    elif model_key == 'GPT-4 Turbo':
        model_name = MODELS[model_key]
        chat = ChatOpenAI(
            openai_api_key=secrets['openai_api_key'],
            model=model_name
        )
        messages = [
            ChatMessage(role="assistant", content="How can I help you?"),
            ChatMessage(role="user", content=prompt)
        ]
        response = await chat.ainvoke(messages)
        return response.content
    
async def update_tab(tab, prompt, result, empty):
    empty.empty()
    tab.chat_message("user").markdown(prompt)
    tab.chat_message("user").markdown(result)

async def main():
    show_sidebar()
    show_intro()

    tabs = st.tabs(MODELS.keys())
    empties = []
    prompt = st.chat_input("Your prompt")
    if prompt:
        tasks = []
        for i, model_key in enumerate(MODELS.keys()):
            with tabs[i]:
                empty = st.empty()
                with empty:
                    if not "soon" in model_key:
                        st.status(f"Getting response from {model_key}...")
                    else:
                        st.write(f"{model_key} is coming soon")
                empties.append(empty)

            async def update_tab_coroutine(tab, prompt, result, empty):
                await update_tab(tab, prompt, result, empty)

            if not "soon" in model_key:
                task = asyncio.create_task(get_response(model_key, prompt))
                task.add_done_callback(
                    lambda t, i=i: asyncio.create_task(
                        update_tab_coroutine(tabs[i], prompt, t.result(), empties[i])
                    )
                )
                tasks.append(task)
        await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
